{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka & Spark Streaming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'  \n",
    "\n",
    "from pyspark import SparkContext  \n",
    "from pyspark.streaming import StreamingContext  \n",
    "from pyspark.streaming.kafka import KafkaUtils \n",
    "import happybase\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import avro.schema\n",
    "import io, random\n",
    "from avro.io import DatumReader\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    pat1 = r'@[A-Za-z0-9]+'\n",
    "    pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "    combined_pat = r'|'.join((pat1, pat2))\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    lower_case = lower_case.replace(\"rt\", \"\")\n",
    "    words = word_tokenize(lower_case)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "        \n",
    "    return \" \".join(filtered_words).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decode a tweet (Avro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tweet(msg):\n",
    "    schema_path =\"twitter.avsc\"\n",
    "    schema = avro.schema.Parse(open(schema_path).read())\n",
    "    bytes_reader = io.BytesIO(msg)\n",
    "    decoder = avro.io.BinaryDecoder(bytes_reader)\n",
    "    reader = avro.io.DatumReader(schema)\n",
    "    tweet = reader.read(decoder)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a tweet to HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"localhost\"\n",
    "table_name = \"tweets\"\n",
    "\n",
    "def save_tweet(tweet):\n",
    "    connection = happybase.Connection(server)\n",
    "    tweets_table = connection.table('tweets')\n",
    "    neg_count_table = connection.table('neg_counter')\n",
    "    pos_count_table = connection.table('pos_counter')\n",
    "    \n",
    "    words = word_tokenize(tweet['cleaned_text'])\n",
    "    \n",
    "    for w in words:\n",
    "        if tweet['target'] == 'positive':\n",
    "            pos_count_table.counter_inc(w, b'info:counter', value=1)\n",
    "        elif tweet['target'] == 'negative':\n",
    "            neg_count_table.counter_inc(w, b'info:counter', value=1)\n",
    "            \n",
    "    tweets_table.put(str(tweet[\"id\"]), {b'tweet:text': tweet[\"text\"], b'tweet:cleaned_text': tweet[\"cleaned_text\"], b'tweet:target': tweet[\"target\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(text):\n",
    "    analysis = TextBlob(text)\n",
    "    \n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets):\n",
    "    if tweets.isEmpty():\n",
    "        return\n",
    "    \n",
    "    for tweet in tweets.collect():\n",
    "        tweet[\"cleaned_text\"] = clean_tweet(tweet[\"text\"])\n",
    "        tweet[\"target\"] = predict_tweet(tweet[\"cleaned_text\"])\n",
    "        save_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloudera/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/MKMaFjsyr6\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/cloudera/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"https://t.co/IhgIMgJfTH\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "batch_duration = 10 # 10 seconds\n",
    "topic = 'twitter' # kafka topic\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"SentimentAnalysisWithSpark\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc = StreamingContext(sc, batch_duration) \n",
    "kafkaStream = KafkaUtils.createStream(ssc, 'localhost:2181', 'spark-streaming', {topic: 1}, valueDecoder=decode_tweet)\n",
    "\n",
    "parsed = kafkaStream.map(lambda t: t[1])\n",
    "parsed.foreachRDD(process_tweets)\n",
    "\n",
    "ssc.start()  \n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
